{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import interp_tools.saes.jumprelu_sae as jumprelu_sae\n",
    "import interp_tools.model_utils as model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.69s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/gemma-2-2b\"\n",
    "dtype = torch.bfloat16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 20\n",
    "\n",
    "repo_id = \"google/gemma-scope-2b-pt-res\"\n",
    "filename = f\"layer_{layer}/width_16k/average_l0_71/params.npz\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "\n",
    "sae = jumprelu_sae.load_gemma_scope_jumprelu_sae(repo_id, filename, layer, model_name, device, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "test_input = \"The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science\"\n",
    "\n",
    "input = tokenizer(test_input, return_tensors=\"pt\", add_special_tokens=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "print(input['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 2304])\n",
      "torch.Size([1, 32, 16384])\n",
      "torch.Size([1, 32, 2304])\n"
     ]
    }
   ],
   "source": [
    "submodule = model_utils.get_submodule(model, layer)\n",
    "\n",
    "acts_BLD = model_utils.collect_activations(model, submodule, input)\n",
    "print(acts_BLD.shape)\n",
    "\n",
    "encoded_acts_BLF = sae.encode(acts_BLD)\n",
    "print(encoded_acts_BLF.shape)\n",
    "\n",
    "decoded_acts_BLD = sae.decode(encoded_acts_BLF)\n",
    "print(decoded_acts_BLD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7019,   29,   97,   68,   68,   96,   69,   59,   65,   80],\n",
      "       device='cuda:0') As we can see, the L0 norm is very high for the first BOS token, so we'll skip it.\n",
      "mean l0: 73.61289978027344\n",
      "frac_variance_explained: 0.7421875\n"
     ]
    }
   ],
   "source": [
    "l0_BL = (encoded_acts_BLF > 0).sum(dim=-1)\n",
    "print(l0_BL[0, :10], \"As we can see, the L0 norm is very high for the first BOS token, so we'll skip it.\")\n",
    "\n",
    "mean_l0 = l0_BL[:, 1:].float().mean()\n",
    "print(f\"mean l0: {mean_l0.item()}\")\n",
    "\n",
    "total_variance = torch.var(acts_BLD[:, 1:], dim=1).sum()\n",
    "residual_variance = torch.var(acts_BLD[:, 1:] - decoded_acts_BLD[:, 1:], dim=1).sum()\n",
    "frac_variance_explained = (1 - residual_variance / total_variance)\n",
    "print(f\"frac_variance_explained: {frac_variance_explained.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
